{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio processing, Video processing and Computer vision\n",
    "## Bachelor in Data Science and Engineering - Universidad Carlos III de Madrid\n",
    "\n",
    "# LAB SESSION 4: IMAGE CLASSIFICATION WITH CNNs\n",
    "\n",
    "# AUTOMATIC DIAGNOSTIC SYSTEM OF SKIN LESSIONS FROM DERMOSCOPIC IMAGES\n",
    "\n",
    "\n",
    "### Iván González Díaz\n",
    "\n",
    "\n",
    "<center><img src='http://www.tsc.uc3m.es/~igonzalez/images/logo_uc3m_foot.jpg' width=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this practice we are going to build a skin lesion diagnosis system based on dermoscopic image analysis.\n",
    "\n",
    "## Part 0: The problem\n",
    "\n",
    "Before starting the practice, we will briefly describe the database that we will use and the problem we aim to address:\n",
    "\n",
    "Our goal is to develop a CNN providing an automatic diagnosis of cutaneous diseases from dermoscopic images. Dermoscopy is a non-invasive technique that allows the evaluation of the colors and microstructures of the epidermis, the dermoepidermal joint and the papillary dermis that are not visible to the naked eye. These structures are specifically correlated with histological properties of the lesions. Identifying specific visual patterns related to color distribution or dermoscopic structures can help dermatologists decide the malignancy of a pigmented lesion. The use of this technique provides a great help to the experts to support their diagnosis. However, the complexity of its analysis limits its application to experienced clinicians or dermatologists.\n",
    "\n",
    "In our scenario, we will consider 3 classes of skin lesions:\n",
    "\n",
    "- Malignant melanoma: Melanoma, also known as malignant melanoma, is the most common type of cancer, and arises from pigmented cells known as melanocytes. Melanomas typically occur on the skin and rarely elsewhere such as the mouth, intestines, or eye.\n",
    "\n",
    "- Seborrheic keratosis: it is a noncancerous (benign) tumor of the skin that originates from the cells of the outer layer of the skin (keranocytes), so it is a non-melanocytic lesion.\n",
    "\n",
    "- Benign nevus: a benign skin tumor caused by melanocytes (it is melanocytic)\n",
    "\n",
    "Figure 1 shows a visual example of the 3 considered lesions:\n",
    "\n",
    "![Image of ISIC](http://www.tsc.uc3m.es/~igonzalez/images/ISIC.jpg)\n",
    "\n",
    "The dataset has been obtained from the 'Internatial Skin Imaging Collaboration' (ISIC) file. It contains 2750 images divided into 3 sets:\n",
    "- Training set: 2000 images\n",
    "- Validation set: 150 images\n",
    "- Test set: 600 images\n",
    "\n",
    "For each clinical case, two images are available:\n",
    "- The dermoscopic image of the lesion (in the ‘images’ folder).\n",
    "- A binary mask with the segmentation between injury (mole) and skin (in the 'masks' folder)\n",
    "\n",
    "Additionally, there is a csv file for each dataset (training, validation and test) in which each lines corresponds with a clinical case, defined with two fields separated by commas:\n",
    "- the numerical id of the lesion: that allows to build the paths to the image and mask.\n",
    "- the lesion label: available only for training and validation, being an integer between 0 and 2: 0: benign nevus, 1: malignant melanoma, 2: seborrheic keratosis. In the case of the test set, labels are not available (their value is -1).\n",
    "\n",
    "Students will be able to use the training and validation sets to build their solutions and finally provide the scores associated with the test set. This practice provides a guideliness to build a baseline reference system. To do so, we will learn two fundamental procedures:\n",
    "\n",
    "- 1) Process your own database with pytorch\n",
    "- 2) Fine-tuning a regular network for our diagnostic problem\n",
    "\n",
    "## Part 1: Handling our custom dataset with pytorch\n",
    "Now we are going to study how we can load and process our custom dataset in pytorch. For that end, we are going to use the package ``scikit-image`` for reading images, and the package ``panda`` for reading csv files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform, util\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "from PIL import Image\n",
    "import pdb\n",
    "import random\n",
    "import numpy.random as npr\n",
    "\n",
    "random.seed(42)\n",
    "npr.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to download and decompress the dataset on a local folder:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY TO USE GOOGLE COLAB. Run this code only the first time you run this notebook and then comment these lines\n",
    "#from shutil import copyfile\n",
    "#from google.colab import drive\n",
    "#import os, sys\n",
    "#drive.mount('/content/drive')\n",
    "#copyfile('/content/drive/My Drive/Colab Notebooks/s4/db1.zip', './db1.zip') #Copy db files to our working folder\n",
    "#copyfile('/content/drive/My Drive/Colab Notebooks/s4/db2.zip', './db2.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Run this only once, in the machine where you want to run your code, then comment these lines\n",
    "import zipfile\n",
    "zipPath='./db1.zip' #path of the 1st zip file\n",
    "dataFolder='./data' #We extract files to the current folder\n",
    "with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dataFolder)\n",
    "    \n",
    "zipPath='./db2.zip' #path of the 2nd zip file\n",
    "dataFolder='./data' # We extract files to the current folder\n",
    "with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n",
    "    zip_ref.extractall(dataFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read the indexed file and display data from image 65. The file structure is one row per image of the database, and two fields:\n",
    "- Image ID (a 4-digit sequence, adding 0 to the left side if required)\n",
    "- Label: 0 benign nevus, 1 melanoma, 2 seborrheic keratosis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv('data/dermoscopyDBtrain.csv',header=0,dtype={'id': str, 'label': int})\n",
    "\n",
    "#We show inform\n",
    "n = 65\n",
    "img_id = db.id[65] \n",
    "label = db.label[65]\n",
    "\n",
    "\n",
    "print('Image ID: {}'.format(img_id))\n",
    "print('Label: {}'.format(label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a simple function to show an image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title_str):\n",
    "    if len(image.shape)>2:\n",
    "        plt.imshow(image)\n",
    "    else:\n",
    "        plt.imshow(image,cmap=plt.cm.gray)\n",
    "    plt.title(title_str)        \n",
    "\n",
    "plt.figure()\n",
    "imshow(io.imread(os.path.join('data/images/', img_id + '.jpg' )),'Image %d'%n)\n",
    "plt.figure()\n",
    "imshow(io.imread(os.path.join('data/masks/', img_id + '.png')),'Mask %d'%n)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Dataset\n",
    "\n",
    "The class `` torch.utils.data.Dataset`` is an abstract class that represents a dataset.\n",
    "\n",
    "To create our custom dataset in pytorch we must inherit from this class and overwrite the following methods:\n",
    "\n",
    "- `` __len__`` so that `` len (dataset) `` returns the size of the dataset.\n",
    "- `` __getitem__`` to support indexing `` dataset [i] `` when referring to sample $i$\n",
    "\n",
    "We are going to create the train and test datasets of our diagnostic problem. We will read the csv in the initialization method `` __init__`` but we will leave the explicit reading of the images for the method\n",
    "`` __getitem__``. This approach is more efficient in memory because all the images are not loaded in memory at first, but are read individually when necessary.\n",
    "\n",
    "Our dataset is going to be a dictionary `` {'image': image, 'mask': mask, 'label': label} ``. You can also take an optional `` transform '' argument so that we can add pre-processing and data augmentation techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DermoscopyDataset(Dataset):\n",
    "    \"\"\"Dermoscopy dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir,transform=None,  maxSize=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path al fichero csv con las anotaciones.\n",
    "            root_dir (string): Directorio raíz donde encontraremos las carpetas 'images' y 'masks' .\n",
    "            transform (callable, optional): Transformaciones opcionales a realizar sobre las imágenes.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(csv_file,header=0,dtype={'id': str, 'label': int})\n",
    "        \n",
    "        if maxSize>0:\n",
    "            newDatasetSize=maxSize #maxSize muestras\n",
    "            idx=np.random.RandomState(seed=42).permutation(range(len(self.dataset)))\n",
    "            reduced_dataset=self.dataset.iloc[idx[0:newDatasetSize]]\n",
    "            self.dataset=reduced_dataset.reset_index(drop=True)\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.img_dir = os.path.join(root_dir,'images') \n",
    "        self.mask_dir = os.path.join(root_dir,'masks')\n",
    "        self.transform = transform\n",
    "        self.classes = ['nevus', 'melanoma', 'keratosis']\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        #Leemos la imagen\n",
    "        img_name = os.path.join(self.img_dir,self.dataset.id[idx] + '.jpg')\n",
    "        image = io.imread(img_name)\n",
    "        #Leemos la máscara\n",
    "        mask_name = os.path.join(self.mask_dir,self.dataset.id[idx] + '.png')\n",
    "        mask = io.imread(mask_name)\n",
    "        \n",
    "        sample = {'image': image, 'mask': mask, 'label':  self.dataset.label[idx].astype(dtype=np.long)}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now instantiate the class to iterate over some samples to see what we generate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtrain.csv',\n",
    "                                    root_dir='data')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    sample = train_dataset[i]\n",
    "    print(i, sample['image'].shape, sample['label'])\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(sample['image'])\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "The optional parameter ``maxSize`` in the constructor allows us to subsample the number of images and consequently reduce the size of the dataset. If not set included or maxSize=0, then the dataset will include all the images. This parameter is useful to train models over smaller datasets during hyperparameter validation and design phases. Working with less images reduces the training time at the expense of obtaining results that may deviate from those obtained with the full dataset size. Of course, the larger the training dataset, the more stable results but the larger the training time. Hence, it is up to the students the use of this parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing and Augmentation: Transforms\n",
    "----------\n",
    "\n",
    "In the previously shown examples we can see that the size of the images is not the same. This would prevent to train a red convolutional neuronal, as the vast majority require fixed-size inputs. Furthermore, the image is not always adjusted to the lesion, and indeed, in some examples lesions are very small compared to the size of the image. It would then be desirable to adjust the input images so that the lesion covers almost the entire image.\n",
    "\n",
    "To do this, we are going to create some preprocessing code, focusing on 5 transformations:\n",
    "\n",
    "- `` CropByMask``: to crop the image using the lesion mask\n",
    "- `` Rescale``: to scale the image\n",
    "- `` RandomCrop``: to crop the image randomly, it allows us to augment the data samples with random crops\n",
    "-  ``CenterCrop``: to perform a central crop of the image with the indicated size (useful in test)\n",
    "- `` ToTensor``: to convert numpy matrices into torch tensors (rearranging the axes).\n",
    "\n",
    "We will define them as callable classes instead of simple functions, as we will not need to pass the transform  parameters every time we call a method. To do this, we only have to implement the `` __call__`` method and, if necessary, the `` __init__`` method.\n",
    "Then we can use a transformation with the following code:\n",
    "\n",
    "::\n",
    "\n",
    "    tsfm = Transform(params)\n",
    "    transformed_sample = tsfm(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropByMask(object):\n",
    "    \"\"\"Crop the image using the lesion mask.\n",
    "\n",
    "    Args:\n",
    "        border (tuple or int): Border surrounding the mask. We dilate the mask as the skin surrounding \n",
    "        the lesion is important for dermatologists.\n",
    "        If it is a tuple, then it is (bordery,borderx)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, border):\n",
    "        assert isinstance(border, (int, tuple))\n",
    "        if isinstance(border, int):\n",
    "            self.border = (border,border)\n",
    "        else:\n",
    "            self.border = border\n",
    "            \n",
    "    def __call__(self, sample):\n",
    "        image, mask, label = sample['image'], sample['mask'],sample['label']\n",
    "        h, w = image.shape[:2]\n",
    "        #Compute the coordinates of the bounding box that contains the mask \n",
    "        sidx=np.nonzero(mask)\n",
    "        minx=np.maximum(sidx[1].min()-self.border[1],0)\n",
    "        maxx=np.minimum(sidx[1].max()+1+self.border[1],w)\n",
    "        miny=np.maximum(sidx[0].min()-self.border[0],0)\n",
    "        maxy=np.minimum(sidx[0].max()+1+self.border[1],h)\n",
    "        #Crop the image\n",
    "        image=image[miny:maxy,minx:maxx,...]\n",
    "        mask=mask[miny:maxy,minx:maxx]\n",
    "\n",
    "        return {'image': image, 'mask': mask, 'label' : label}\n",
    "    \n",
    "class Rescale(object):\n",
    "    \"\"\"Re-scale image to a predefined size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): The desired size. If it is a tuple, output is the output_size. \n",
    "        If it is an int, the smallest dimension will be the output_size\n",
    "            a we will keep fixed the original aspect ratio.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask, label = sample['image'], sample['mask'],sample['label']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        msk = transform.resize(mask, (new_h, new_w))\n",
    "\n",
    "        return {'image': img, 'mask': msk, 'label' : label}\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Randomly crop the image.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Crop size. If  int, square crop\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask, label = sample['image'], sample['mask'],sample['label']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        if h>new_h:\n",
    "            top = np.random.randint(0, h - new_h)\n",
    "        else:\n",
    "            top=0\n",
    "            \n",
    "        if w>new_w: \n",
    "            left = np.random.randint(0, w - new_w)\n",
    "        else:\n",
    "            left = 0\n",
    "            \n",
    "        image = image[top: top + new_h,\n",
    "                     left: left + new_w]\n",
    "\n",
    "        mask = mask[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "\n",
    "        return {'image': image, 'mask': mask, 'label': label}\n",
    "    \n",
    "class CenterCrop(object):\n",
    "    \"\"\"Crop the central area of the image\n",
    "\n",
    "    Args:\n",
    "        output_size (tupla or int): Crop size. If int, square crop\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask, label = sample['image'], sample['mask'],sample['label']\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        rem_h = h - new_h\n",
    "        rem_w = w - new_w\n",
    "        \n",
    "        if h>new_h:\n",
    "            top = int(rem_h/2)\n",
    "        else:\n",
    "            top=0\n",
    "            \n",
    "        if w>new_w: \n",
    "            left = int(rem_w/2)\n",
    "        else:\n",
    "            left = 0\n",
    "            \n",
    "        image = image[top: top + new_h,\n",
    "                     left: left + new_w]\n",
    "\n",
    "        mask = mask[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "\n",
    "        return {'image': image, 'mask': mask, 'label': label}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays into pytorch tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask, label = sample['image'], sample['mask'],sample['label']\n",
    "\n",
    "        # Cambiamos los ejes\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = torch.from_numpy(image)\n",
    "        # A la máscara le añadimos una dim fake al principio\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        label=torch.tensor(label,dtype=torch.long)\n",
    "        \n",
    "        return {'image':image,\n",
    "                'mask':mask,\n",
    "                'label':label}\n",
    "    \n",
    "class Normalize(object):\n",
    "    \"\"\"Normalize data by subtracting means and dividing by standard deviations.\n",
    "\n",
    "    Args:\n",
    "        mean_vec: Vector with means. \n",
    "        std_vec: Vector with standard deviations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean,std):\n",
    "      \n",
    "        assert len(mean)==len(std),'Length of mean and std vectors is not the same'\n",
    "        self.mean = np.array(mean)\n",
    "        self.std = np.array(std)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask, label = sample['image'], sample['mask'],sample['label']\n",
    "        c, h, w = image.shape\n",
    "        assert c==len(self.mean), 'Length of mean and image is not the same' \n",
    "        dtype = image.dtype\n",
    "        mean = torch.as_tensor(self.mean, dtype=dtype, device=image.device)\n",
    "        std = torch.as_tensor(self.std, dtype=dtype, device=image.device)\n",
    "        image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    \n",
    "        \n",
    "        return {'image': image, 'mask': mask, 'label' : label}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Use the following code to apply the previous transforms and experiment with the values of the parameters to study their influence. Read the documentation at the begining of each class to understand the sintaxis of the input parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modify this code to make your experiments....\n",
    "\n",
    "cmask = CropByMask(15)\n",
    "scale = Rescale(256)\n",
    "rcrop = RandomCrop(224)\n",
    "ccrop = CenterCrop(224)\n",
    "\n",
    "# Apply each of the above transforms on sample.\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "sample = train_dataset[65]\n",
    "ax = plt.subplot(3,2, 1)\n",
    "plt.tight_layout()\n",
    "ax.set_title('original')\n",
    "plt.imshow(sample['image'])\n",
    "    \n",
    "for i, tsfrm in enumerate([cmask, scale, ccrop, rcrop]):\n",
    "    transformed_sample = tsfrm(sample)\n",
    "\n",
    "    ax = plt.subplot(3, 2, i+2)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(type(tsfrm).__name__)\n",
    "    plt.imshow(transformed_sample['image'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using package torchvision.transforms\n",
    "\n",
    "Package torchvision.trasforms comes with many useful methods that implement valuable transforms for data augmentation. Those transforms can be applied either to torch.tensors or to images. However, torchvision uses PIL library to read and process images, in contrast to the matrix representation of images used in scikit-image library (the library we have used in the previous methods). Furthermore, some of the transforms affect both to input images and skin-lesion masks, which avoid applying them to our samples. Hence, in order to use a torchvision transform we need to implement a pipeline that:\n",
    "\n",
    "- 1) First, convert the matrix-based images to PILImages\n",
    "- 2) Apply the transform to PILImages\n",
    "- 3) Convert PILImages to matrices\n",
    "\n",
    "And applies these to input images and, if necessary, to the binary skin-lesion masks. The following class ``TVCenterCrop`` implements the same functionality as the previous ``CenterCrop``, but using the corresponding method in torchvision. It is therefore a useful example if you plan to make use of torchvision transforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TVCenterCrop(object):\n",
    "    \"\"\"Crop the central area of the image. Example using the method in torchvision. Requires to\n",
    "    internally convert from skimage (numpy array) to PIL Image\n",
    "\n",
    "    Args:\n",
    "        output_size (tupla or int): Crop size. If int, square crop\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.CC=transforms.CenterCrop(size)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask, label = sample['image'], sample['mask'],sample['label']\n",
    "        pil_image=Image.fromarray(util.img_as_ubyte(image))\n",
    "        pil_image=self.CC(pil_image)\n",
    "        image=util.img_as_float(np.asarray(pil_image))\n",
    "        \n",
    "        pil_mask=Image.fromarray(util.img_as_ubyte(mask))\n",
    "        pil_mask=self.CC(pil_mask)\n",
    "        mask=util.img_as_float(np.asarray(pil_mask))\n",
    "        \n",
    "        return {'image': image, 'mask': mask, 'label': label}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the transform to check its behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply each of the above transforms on sample.\n",
    "fig = plt.figure()\n",
    "sample = train_dataset[65]\n",
    "ax = plt.subplot(1,2, 1)\n",
    "plt.tight_layout()\n",
    "ax.set_title('original')\n",
    "plt.imshow(sample['image'])\n",
    "\n",
    "tvcc=TVCenterCrop(256)\n",
    "\n",
    "transformed_sample = tvcc(sample)\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.tight_layout()\n",
    "ax.set_title('Center Crop transform using torchvision')\n",
    "plt.imshow(transformed_sample['image'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Implement other transforms using torchvision package. Take TVCenterCrop as guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here....\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composed Transforms\n",
    "\n",
    "Now let's apply the different transformations to our images. \n",
    "\n",
    "We will rescale the images so that their smallest dimension is 224 and then make random crops of size 224. To compose the transformations ``Rescale`` and ``RandomCrop`` we can use ``torchvision.transforms.Compose``, which is a simple callable class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = transforms.Compose([CropByMask(15), Rescale(224),RandomCrop(224)])\n",
    "\n",
    "# Apply each of the above transforms on sample.\n",
    "fig = plt.figure()\n",
    "sample = train_dataset[65]\n",
    "ax = plt.subplot(1,2, 1)\n",
    "plt.tight_layout()\n",
    "ax.set_title('original')\n",
    "plt.imshow(sample['image'])\n",
    "    \n",
    "transformed_sample = composed(sample)\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.tight_layout()\n",
    "ax.set_title('Composed transform')\n",
    "plt.imshow(transformed_sample['image'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Use the previous code to test different combinations of transform and discuss the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Data augmentation techniques are useful as long as they model random transforms that may happen in the real world, and so can be found in test. Hence, the choice of the data augmentation techniques to be applied is problem-dependent and may differ from one dataset to another. Have a look at the images in the dataset and try to figure out which transforms are appropriate for these problem. Imagine how a dermatologist takes a dermoscopic picture and which factors may differ from one capture to another. Your results in the challenge will strongly depend on your design!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code here....\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating the dataset\n",
    "-----------------------------\n",
    "\n",
    "We can now put everything together to create the train and test datasets with the corresponding transformations.\n",
    "In summary, every time we sample an image from the dataset (during training):\n",
    "- We will read the image and the mask\n",
    "- We will apply the transformations and we will crop the image using a bounding box computed from the mask\n",
    "- As the final cropping operation is random, we perform data augmentation during sampling\n",
    "\n",
    "We can easily iterate over the dataset with a ``for i in range`` loop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pixel means and stds expected by models in torchvision\n",
    "pixel_mean=[0.485, 0.456, 0.406]\n",
    "pixel_std=[0.229, 0.224, 0.225]\n",
    "\n",
    "#Train Dataset\n",
    "train_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtrain.csv',\n",
    "                                    root_dir='data',\n",
    "                                    maxSize=500, ###IMPORTANT: maxSize=500 to speed-up the training process\n",
    "                                    transform=transforms.Compose([\n",
    "                                    CropByMask(15),\n",
    "                                    Rescale(224),\n",
    "                                    RandomCrop(224),\n",
    "                                    ToTensor(),\n",
    "                                    Normalize(mean=pixel_mean,\n",
    "                                    std=pixel_std)\n",
    "                                    ]))\n",
    "#Val dataset\n",
    "val_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBval.csv',\n",
    "                                    root_dir='data',\n",
    "                                    transform=transforms.Compose([\n",
    "                                    CropByMask(15),\n",
    "                                    Rescale(224),\n",
    "                                    CenterCrop(224),\n",
    "                                    ToTensor(),\n",
    "                                    Normalize(mean=pixel_mean,\n",
    "                                    std=pixel_std)\n",
    "                                    ]))\n",
    "\n",
    "#Test dataset\n",
    "test_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtest.csv',\n",
    "                                    root_dir='data',\n",
    "                                    transform=transforms.Compose([\n",
    "                                    CropByMask(15),\n",
    "                                    Rescale(224),\n",
    "                                    CenterCrop(224),\n",
    "                                    ToTensor(),\n",
    "                                    Normalize(mean=pixel_mean,\n",
    "                                    std=pixel_std)\n",
    "                                    ]))\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    sample = train_dataset[i]\n",
    "\n",
    "    print(i, sample['image'].size(), sample['label'])\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to create a dataloader allowing to:\n",
    "\n",
    "- Sample batches of samples to feed the network during training\n",
    "- Shuffle data\n",
    "- Load the data in parallel using multiple cores.\n",
    "\n",
    "``torch.utils.data.DataLoader`` is an iterator that provides all these features. An important parameter of the iterator is ``collate_fn``. We can specify how samples are organized in batches by choosing the most appropriate function. In any case, the default option should work fine in most cases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Specify training dataset, with a batch size of 8, shuffle the samples, and parallelize with 4 workers\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16,\n",
    "                        shuffle=True, num_workers=4)\n",
    "#Validation dataset => No shuffle\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16,\n",
    "                        shuffle=False, num_workers=4)\n",
    "\n",
    "#Test Dataset => => No shuffle\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16,\n",
    "                        shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# Auxiliary function to visualize a batch\n",
    "def show_batch(sample_batched):\n",
    "    \"\"\"Mostramos las lesiones de un batch.\"\"\"\n",
    "    images_batch, labels_batch = \\\n",
    "            sample_batched['image'], sample_batched['label']\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "    grid_border_size = 2\n",
    "    \n",
    "    #Generamos el grid\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    #Lo pasamos a numpy y lo desnormalizamos\n",
    "    grid=grid.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    grid = std * grid + mean\n",
    "    grid = np.clip(grid, 0, 1)\n",
    "    plt.imshow(grid)\n",
    "    plt.title('Batch from dataloader')\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['label'])\n",
    "    plt.figure()\n",
    "    show_batch(sample_batched)\n",
    "    plt.axis('off')\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "        \n",
    "    #We show the data of the 3rd batch and stop.\n",
    "    if i_batch == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fine-tuning a pre-trained model\n",
    "\n",
    "In the second part of the practice we will build an automatic skin lesion diagnosis system. Instead of training a CNN designed by us from the beginning, we will fine-tune a network that has previously been trained for another task. As seen in the lectures, this usually becomes a good alternative when we do not have many data in the training dataset (in relation to the parameters to be learned).\n",
    "\n",
    "In particular, we will use the Alexnet CNN, included in the ``torchvision`` package.\n",
    "\n",
    "### Performance Metric for evaluation\n",
    "We will start by defining the metric we will use to evaluate our network. In particular, and following the instructions of the organizers of the original ISIC challenge, we will use the area under the ROC or AUC, but we will calculate 3 different AUCs:\n",
    "- 1) AUC of binary problem melanoma vs all\n",
    "- 2) AUC of the binary problem seborrheic keratosis vs all\n",
    "- 3) AUC average of the previous two\n",
    "\n",
    "The following function computes AUCs from the complete database outputs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that computes 2 AUCs: melanoma vs all and keratosis vs all\n",
    "# scores is nx3: n is the number of samples in the dataset \n",
    "# labels is nx1\n",
    "# Function resturns an array with two elements: the auc values\n",
    "def computeAUCs(scores,labels):\n",
    "                \n",
    "    aucs = np.zeros((2,))\n",
    "    #Compute the AUC of melanoma vs all\n",
    "    scores_mel = scores[:,1]\n",
    "    labels_mel = (labels == 1).astype(np.int) \n",
    "    aucs[0]=metrics.roc_auc_score(labels_mel, scores_mel)\n",
    "\n",
    "    #Compute the AUC of keratosis vs all\n",
    "    scores_sk = scores[:,2]\n",
    "    labels_sk = (labels == 2).astype(np.int) \n",
    "    aucs[1]=metrics.roc_auc_score(labels_sk, scores_sk)\n",
    "    \n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function\n",
    "\n",
    "We continue defining the function to train our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model parameters are the network (model), the criterion (loss),\n",
    "# the optimizer, a learning scheduler (una estrategia de lr strategy), and the training epochs\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    numClasses = len(image_datasets['train'].classes)\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_aucs = np.zeros((2,)) #AUCs melanoma vs all, and keratosis\n",
    "    best_auc = 0\n",
    "    best_epoch = -1\n",
    "    \n",
    "    #Loop of epochs (each iteration involves train and val datasets)\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Cada época tiene entrenamiento y validación\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set the model in training mode\n",
    "            else:\n",
    "                model.eval()   # Set the model in val mode (no grads)\n",
    "\n",
    "            \n",
    "            #Dataset size\n",
    "            numSamples = dataset_sizes[phase]\n",
    "            \n",
    "            # Create variables to store outputs and labels\n",
    "            outputs_m=np.zeros((numSamples,numClasses),dtype=np.float)\n",
    "            labels_m=np.zeros((numSamples,),dtype=np.int)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            contSamples=0\n",
    "            \n",
    "            # Iterate (loop of batches)\n",
    "            for sample in dataloaders[phase]:\n",
    "                inputs = sample['image'].to(device).float()\n",
    "                labels = sample['label'].to(device)\n",
    "                \n",
    "                \n",
    "                #Batch Size\n",
    "                batchSize = labels.shape[0]\n",
    "                \n",
    "                # Set grads to zero\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Register ops only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward & parameters update only in train\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # Accumulate the running loss\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                #Apply a softmax to the output\n",
    "                outputs=F.softmax(outputs.data,dim=1)\n",
    "                # Store outputs and labels \n",
    "                outputs_m [contSamples:contSamples+batchSize,...]=outputs.cpu().numpy()\n",
    "                labels_m [contSamples:contSamples+batchSize]=labels.cpu().numpy()\n",
    "                contSamples+=batchSize\n",
    "                \n",
    "            #At the end of an epoch, update the lr scheduler    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            #Accumulated loss by epoch\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            \n",
    "            #Compute the AUCs at the end of the epoch\n",
    "            aucs=computeAUCs(outputs_m,labels_m)\n",
    "            \n",
    "            #And the Average AUC\n",
    "            epoch_auc = aucs.mean()\n",
    "                         \n",
    "            print('{} Loss: {:.4f} AUC mel: {:.4f} sk: {:.4f} avg: {:.4f}'.format(\n",
    "                phase, epoch_loss, aucs[0], aucs[1], epoch_auc))\n",
    "\n",
    "            # Deep copy of the best model\n",
    "            if phase == 'val' and epoch_auc > best_auc:\n",
    "                best_auc = epoch_auc\n",
    "                best_aucs = aucs.copy()        \n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_epoch = epoch\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best model in epoch {:d} val AUCs: mel {:4f} sk {:4f} avg {:4f}'.format(best_epoch,best_aucs[0],best_aucs[1],best_auc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning of a pre-trained CNN\n",
    "Once we have defined the training and evaluation functions, we will fine-tune AlexNet CNN using our database. In addition, we define the loss, the optimizer and the lr scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_ft = models.alexnet(pretrained=True)\n",
    "\n",
    "# We need to set-up the output layer (fully connected) to provide 3 scores (nevus, melanoma, y queratosis).\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "num_ftrs = model_ft.classifier[6].in_features\n",
    "model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "#COnvert netowrk to GPU if available\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "#The loss is a cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# We will use SGD with momentum as optimizer\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "# Our scheduler starts with an lr=1e-3 and decreases by a factor of 0.1 every 7 epochs.\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up the data loaders\n",
    "No we will assign the dataloaders over the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {'train' : train_dataset, 'val': val_dataset}\n",
    "\n",
    "dataloaders = {'train' : train_dataloader, 'val': val_dataloader}\n",
    "          \n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "class_names = image_datasets['train'].classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment:\n",
    "\n",
    "The current code trains the network for 10 epochs using a SGD with momentum, with a learning rate step strategy, and an early stop strategy (best model selection in validation). You can modify this configuration by using other parameters, optimizers, schedulers, ... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "Current solution takes an Alexnet model pretrained on Imagenet and then fine tunes the network in our data. You can try other networks that come with pytorch. For each network, you will have to adapt the last layers to work with the 3 classes present in our problem. You have a guideliness at:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "\n",
    "Apart from those models in tochvision, github and other repositories contain many networks that have provided competitive results in image classification problems. You can have a look to external implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "The current code trains the network for 10 epochs and then returns the model corresponding to the best epoch in the validation set (measured by the average AUC). It might be interesting, especially if you plan to work with Google Collab (which has very limited session lengths), to implement a save-and-resume strategy that allows you to train your models in different sessions. You have details of how to implement it at:\n",
    "\n",
    "https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluation (Important)\n",
    "The evaluation of this practice will be done through a challenge. For this, students are asked to provide the following:\n",
    "- Two submissions for the test set scores (unnormalized), each one represented by a 600x3 matrix where 600 is the number of test samples, and 3 are the classes considered in the problem. The matrix must be provided in csv format (with 3 numbers per row separated by ',').\n",
    "\n",
    "- In addition, students will submit a short report (1 side at most for the description, plus 1 side for references and 1 for figures, if necessary) where they will describe the most important aspects of the proposed solution and include a table with the validation results achieved by their extensions/decisions. The objective of this report is for the teacher to assess the developments / extensions / decisions made by the students when optimizing their system. And the table is asked to demonstrate that, at least in validation, their decisions helped to improve the system performance.  You don't need to provide an absolute level of detail about the changes made, just list them, briefly discuss their purpose and show their impact in the table.\n",
    "\n",
    "The deadline for delivery of the results file and the report is **Wednesday Nov 2 at 23:59**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we provide some functions that allow to test the network and create the csv file with the outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code that generates the test matrix\n",
    "def test_model(model):\n",
    "    since = time.time()\n",
    "    \n",
    "    numClasses = len(test_dataset.classes)\n",
    "    \n",
    "    model.eval()   # Ponemos el modelo en modo evaluación\n",
    "\n",
    "    #Tamaño del dataset\n",
    "    numSamples = len(test_dataset)\n",
    "            \n",
    "    # Creamos las variables que almacenarán las salidas y las etiquetas\n",
    "    outputs_m=np.zeros((numSamples,numClasses),dtype=np.float)\n",
    "    labels_m=np.zeros((numSamples,),dtype=np.int)\n",
    "    contSamples=0\n",
    "            \n",
    "    # Iteramos sobre los datos\n",
    "    for sample in test_dataloader:\n",
    "        inputs = sample['image'].to(device).float()\n",
    "                \n",
    "                \n",
    "        #Tamaño del batch\n",
    "        batchSize = inputs.shape[0]\n",
    "                \n",
    "        # Paso forward\n",
    "        with torch.torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "            #Aplicamos un softmax a la salida\n",
    "            outputs=F.softmax(outputs.data,dim=1)\n",
    "            outputs_m [contSamples:contSamples+batchSize,...]=outputs.cpu().numpy()\n",
    "            contSamples+=batchSize\n",
    "                \n",
    "            \n",
    "    return outputs_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the previous function and obtaining the matrix with the scores (Nx3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=test_model(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally save the matrix into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('output_test.csv', mode='w') as out_file:\n",
    "    csv_writer = csv.writer(out_file, delimiter=',')\n",
    "    csv_writer.writerows(outputs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
