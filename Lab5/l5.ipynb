{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1652794398595,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "bukWo1QXbWE2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "code_path='.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuKZySPHbWFB"
   },
   "source": [
    "## Audio Processing, Video Processing and Computer Vision\n",
    "## Grado en Ciencia e Ingeniería de Datos\n",
    "\n",
    "# LAB SESSION 5: OBJECT DETECTION WITH CNNs\n",
    "\n",
    "\n",
    "### Miguel Molina Moreno\n",
    "\n",
    "\n",
    "<center><img src='http://www.tsc.uc3m.es/~igonzalez/images/logo_uc3m_foot.jpg' width=400 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSazWDcebWFC"
   },
   "source": [
    "## Objectives \n",
    "\n",
    "The main goal of this session is that the student investigates a Region-Convolutional Neural Network (R-CNN) that incorproates a Region Proposal Network (RPN) in the context of the object detection task. In other words, over a certain image, we will classify certain objects and their locations based on a set of predefined classes.\n",
    "\n",
    "Particularly, we will work with Faster-RCNN [1], the first network to use an automatic strategy for region proposal. We will use part of the PASCAL VOC dataset to conduct our experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAteaecfbWFH"
   },
   "source": [
    "## References\n",
    "\n",
    "- [1] Faster-RCNN. https://arxiv.org/abs/1706.05587\n",
    "- [2] PASCAL VOC 2012 dataset. http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html\n",
    "- [3] https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection\n",
    "- [4] https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/\n",
    "- [5] Fast-RCNN y ROIPooling: https://arxiv.org/abs/1504.08083\n",
    "- [6] https://en.wikipedia.org/wiki/File:RoI_pooling_animated.gif\n",
    "- [7] FPN: https://arxiv.org/abs/1612.03144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0msY25S5xDsw"
   },
   "source": [
    "## Submission and Evaluation\n",
    "\n",
    "The submission required for this lab is an executed version of this notebook (all outputs must be visible) with the required implementations and answers. Please use figures and images to properly support and justify your claims. \n",
    "\n",
    "__*Please make sure that the results can be visualized before submitting the notebook to AulaGlobal.*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDa2gUbNbWFI"
   },
   "source": [
    "## Before we begin\n",
    "\n",
    "Some previous steps need to be taken for the use of Google Colaboratory. The material of the lab needs to be unzipped into the Google Drive directory where this notebook is located. The name of this directory needs to be `objdetection`. Once this is done, the following code needs to be run after properly specifying the path to the directory (variable `path`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27072,
     "status": "ok",
     "timestamp": 1652794425662,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "T9kpu0KmbWFK",
    "outputId": "ea7c8e72-621e-46a5-a328-378399e5f1f5"
   },
   "outputs": [],
   "source": [
    "#uncomment only if you plan to execute this in Google Colab\n",
    "#from google.colab import drive\n",
    "#import os, sys\n",
    "#from shutil import copyfile\n",
    "\n",
    "#drive.mount('/content/drive')\n",
    "#print(os.getcwd())\n",
    "#code_path = 'YOURPATH' # <-- Full path prefix to objdetection dir\n",
    "#sys.path.append(code_path) \n",
    "\n",
    "#copyfile(code_path + '/myVOC4.zip', './myVOC4.zip') #Copy db files to our working folder\n",
    "\n",
    "# Ignore warnings (when debugging, comment the next two lines)\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIsPUaHcxuuk"
   },
   "source": [
    "Now, we will decompress the dataset to a local folder to optimize data loading speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 884,
     "status": "ok",
     "timestamp": 1652794426537,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "7xYXOlZjxtWt"
   },
   "outputs": [],
   "source": [
    "#NOTE: Run this only once, in the machine where you want to run your code, then comment these lines\n",
    "#import zipfile\n",
    "#zipPath='./myVOC4.zip' #path of the 1st zip file\n",
    "#dataFolder='.' #We extract files to the current folder\n",
    "#with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n",
    "#    zip_ref.extractall(dataFolder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xs4jbTSCbWFQ"
   },
   "source": [
    "Additionally, to run the code in a GPU environment (recommended) go to `Entorno de ejecución->Cambiar tipo entorno de ejecución` and choose GPU in `acelerador por hardware`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "883Ll5P8bWFR"
   },
   "source": [
    "## Part 1. Theory.\n",
    "\n",
    "Before we learn how a Faster RCNN is implemented, let us remember some key aspects from the theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkXSBkRsbWFS"
   },
   "source": [
    "### Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KMr3sApbWFT"
   },
   "source": [
    "In object detection, an object is defined with respect to a _bounding box_, which completely surounds the object. The bounding box is defined with respect to its location (usually its center or top-left coordinate) and its size (width and length). This means that 4 numbers are required to do so (see image below). Usually, the 4 coordinates are normalized by the image dimensions, so that they can take values in the range [0,1].\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/plasavall/detection_images/main/cs.PNG\">\n",
    "\n",
    "Each object will also have an associated label depending on which predefined category it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2BKQdz9bWFV"
   },
   "source": [
    "### Faster-RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNg1OxUibWFV"
   },
   "source": [
    "This section will focus on some key concepts regarding Faster-RCNNs. However, it is highly recommended to review the original paper [1] and the course slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dvc8TgabWFW"
   },
   "source": [
    "Faster-RCNN is a _framework_ that adapts a classification-oriented CNN to object detection. To do so, a _backbone_ is used consisting on the feature extraction layers of a pre-trained CNN. From this point, two  processing blocks are used to simultaneously infer an object location and its associated class. These blocks are a *Region Proposal Network (RPN)*, and a *RoI pooling* layer followed by a classification network (see image below).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/plasavall/detection_images/main/faster.JPEG\">\n",
    "\n",
    "Thus, the specific layers in Faster RCNNs (after the backbone is run) work over a high-level feature map in which both the bounding boxes and the defining features have to be found. This feature map does not have the same size of the image, since strides>1 are used in the backbone layers that reduce the spatial dimension of the input tensor. In particular,  we define a global stride that indicates the accumulated spatial subsampling factor: e.g. if the size of the input image is H x W and the global stride is 16, then the high level feature map that goes to the RPN and Roi Pooling blocks is H/16 x W/16. \n",
    "\n",
    "\n",
    "#### Region Proposal Network (RPN)\n",
    "\n",
    "The main goal of this block is to obtain the areas of the image (or rather, of the feature maps) that are likely to contain an object. Therefore, we consider that each spatial location of the feature map may become the center of _k_ reference bounding boxes, known as _anchors_ in the algorithm. Typically, _k_>1, as we consider various sizes and aspect ratios (e.g. often, k=9, considering 3 sizes: 128, 256 and 512 pixels; and 3 aspect ratios 1:1, 1:2 and 2:1). An example of the anchors' locations and their representations can be seen in the images below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/plasavall/detection_images/main/anchor_centers.png\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/plasavall/detection_images/main/anchors-progress.png\">\n",
    "\n",
    "From the amount of available anchors ($k \\cdot H/16 \\cdot W/16$), only some of them match the actual position and shape of the objects present in the image. Hence, the first task of the RPN, executed for each anchor, to decide if the anchor is positive (it contains an object) or negative (it does not).\n",
    "\n",
    "In order to train the corresponding layers in the RPN, we define a loss function $L_{objectness}$, which is a negative binary cross entropy between the out probabilities of the RPN and the labels. To provide the necessary ground truth labels, we rely on a metric called IoU - _Intersection over Union_ -, which stands for the intersection over union between the bounding box proposed by an anchor and the ground truth bounding box of an object. Anchors with a high IoU are labeled as positive (e.g. IoU>0.7), anchors with a low value (e.g. IoU<0.3) are labeled negative, whereas anchors with ambiguous values of IoU (e.g. 0.3<IoU<0.7) do not contribute to the training stage). See [1] for more details.\n",
    "\n",
    "The number of sizes and shapes in the anchors, however, are limited by design and, hence, insufficient to represent the object's shape precisely. Thus, for the anchors considered as positive (those with high objectness score) the bounding box is then adjusted through a regression loss function $L_{reg}$, which aims to estimate the the difference between the real bounding box and the anchors' one. Therefore, the bounding box finally predicted by the RPN can be seen as a slight deviation of the original one proposed by the anchor.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/plasavall/detection_images/main/reg.PNG\">\n",
    "\n",
    "In summary, $L_{objectness}$ finds the best anchors to represent the obejcts in the image while $L_{reg}$ adapts those anchors to the true shape of the object. \n",
    "\n",
    "\n",
    "#### _RoI pooling_ and classification\n",
    "\n",
    "Up to now, objects have been defined category-agnostically (without considering their class). Once the objects have been located, we need to determine their category. To do this, we need to crop the areas of the feature maps defined by the bounding boxes and pass them to a final set of fully-connected layers. However, this cannot be done directly, as it would require all crops to be the same size, which is almost always false. To address this issue, we use a *RoI pooling* strategy, which adapts the features map to a fixed size that can feed the subsequent layers.\n",
    "\n",
    "This strategy simply consists on defining a grid over the feature map's bounding box (the ROI) which will be used to pool values from. The bounding box's size is $H_{bb}$ x $W_{bb}$, so if we set the grid to be $H_{fix}$ x $W_{fix}$ with each cell of the grid being size $H_{bb}/H_{fix}$ x $W_{bb}/W_{fix}$, we can apply a max pooling from each cell to obtain an adapted, fixed-sized representation of the feature map. This can be seen in the animation below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/plasavall/detection_images/main/RoI_pooling_animated.gif\">\n",
    "\n",
    "Once the fixed-sized representations are computed, we can use the network to classify the objects according to their category. The network is trained using a loss function $L_{cls}$. Additionally, the bounding boxes are refined with the newly obtained information (RoI pooling representation and class categorization) with a new regressive loss function $L_{reg2}$. An example of object detection for a certain input image is shown below (bounding box + object class with associated likelyhood).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/plasavall/detection_images/main/result.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayYVJQDOZKIc"
   },
   "source": [
    "### Additional information for this lab: FPN \n",
    "\n",
    "In object detection networks, detection object at different scales is challenging (particularly for small objects). The traditional strategy for this is the construction of a pyramid for the image at different scales (see the figure below), and the application of the same detection procedure at each scale. However, in the case of neural networks, the processing of different versions of the same image requires both memory and time. Alternatively, a more efficient multiscale processing can be applied to some intermediate feature representations obtained in the backbone network, as they will show notably smaller spatial dimensions.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/plasavall/detection_images/main/pyramid.jpeg\">\n",
    "\n",
    "Note that feature maps near the input are not useful for especially large object detection, as they are composed of low-level structures (edges) of the image. For this reason, frequently, prior to an RPN, a * Feature Pyramid Network * (FPN) is applied, a feature extractor that generates feature maps of different spatial resolutions (thus multi-scale) with different information for each scale, like the figure below shows.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/plasavall/detection_images/main/fpn.png\">\n",
    "\n",
    "On each of these feature maps, the RPN proposes the *anchors*. Specifically, depending on the spatial resolution of each feature map, the RPN uses different translation-invariant *anchors*: the RPN proposes the smallest-size translation-invariant *anchors* on the higher-resolution feature map (bottom layers of the backbone) and the largest translation-invariant *anchors* over the lower resolution map locations (top layers of the backbone). This makes sense from a logical point of view: on maps with more low-level information, the small *anchors* are placed to search for small objects, and on maps with high-level information, the larger *anchors* are placed, which look for objects that make up a large percentage of the image area.\n",
    "\n",
    "For more detailed information, see [7]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5sXEE1mbWFX"
   },
   "source": [
    "### Evaluation for object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bR5YRYv9bWFZ"
   },
   "source": [
    "The performance of the detection system is analyzed in terms of the F1 score. However, to define this measure, we mustfirst introduce the precision and the recall. Each of these measures are defined below.\n",
    "\n",
    "The __precision__, for the case of object detection, represents the percentage of correctly detected objects out of the total number of detected objects (correct and incorrect). It is calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "P=\\frac{TP}{TP+FP},\n",
    "\\end{equation}\n",
    "\n",
    "where $TP$ is the number of *true positives* (correctly identified objects) and $FP$ the number of *false positives* (incorrectly identified objects).\n",
    "\n",
    "The __recall__ represents the percentage of correctly detected objects out of the total number of objects present in the images. It is calculated as:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "R=\\frac{TP}{TP+FN},\n",
    "\\end{equation}\n",
    "\n",
    "where $ FN $ represents the number of *false negatives* (objects present in the images but not detected).\n",
    "\n",
    "Nevertheless, the network provides a series of *bounding boxes* on the image that can overlap with the *ground truth* to a greater or lesser extent. Thus, we have to establish a __criterion to consider a prediction as correct__. This is usually established in the quality of segmentation, by means of the *Intersection over Union*, __IoU__ measure, also called *Jaccard Index* (JI). The $ IoU $ measures the similarity between two regions $A$ and $B$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "IoU=\\frac{A \\cap B}{A \\cup B}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\cap$ is the intersection between the regions (area in common) and $\\cup$ is the union or total area that they cover between them. A minimum threshold of $IoU_{th} = 0.5 $ or $ IoU_ {th} = 0.7$ is usually set to consider a detection as correct. An example of the $ IoU $ measure on *bounding boxes* can be seen below (it will be used in our object detection architecture).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/plasavall/detection_images/main/iou.png\" width=\"500 pix\">\n",
    "\n",
    "Similarly, the $IoU$ measure can be used to set quality in object instance segmentation.\n",
    "\n",
    "Finally, once these measures have been described, the F1 score can be defined. The F1-*score* considers both the precision and the recall to obtain a weighted harmonic mean between them such that:\n",
    "\n",
    "\\begin{equation}\n",
    "F1=2\\cdot \\frac{P\\cdot R}{P+R},\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, this measure summarizes in a single figure the performance of our object detector for a given __point of operation__. The network provides a series of *bounding boxes* with an associated __*score*__ and predicted category, which indicates how likely it is that an object of that certain class exists in that region. However, the precision, the recall and the F1 measure requires hard decisions, which will be generated depending  on the __operation point__ chosen for the system, namely an object detection threshold (defined as th_score later in the notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJGnXdBobWFZ"
   },
   "source": [
    "### The definition of the object detection problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-u9w5oLbWFa"
   },
   "source": [
    "The database that will be used in practice is a subset of the PASCAL VOC 2012 database [2]. This database contains images with various objects belonging to 20 categories:\n",
    "\n",
    "- __Person__: _person_\n",
    "- __Animal__: _bird, cat, cow, dog, horse, sheep_\n",
    "- __Vehicle__: _aeroplane, bicycle, boat, bus, car, motorbike, train_\n",
    "- __Indoor__: _bottle, chair, dining table, potted plant, sofa, tv/monitor_\n",
    "\n",
    "During this session you are going to work on the set __Indoor__. Specifically, objects of the classes: _bottle, chair_ and _dining table_ will be detected. The images are provided in separate folders for training and testing. The sets are distributed as follows:\n",
    "- Training: 234 images, in which there are 90 objects of the *bottle* class, 125 of the *chair* class, 36 of the *dining table* class and 40 of the *sofa* class.\n",
    "- Test: 68 images, in which there are 36 objects of the *bottle* class, 52 of the *chair* class and 15 of the *dining table* class and 27 of the *sofa* class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDLKHGxJbWFb"
   },
   "source": [
    "## Part 2. Implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAJk5jQubWFc"
   },
   "source": [
    "First, we import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3884,
     "status": "ok",
     "timestamp": 1652794430417,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "Wf0s39kvbWFd",
    "outputId": "d2f98bda-e87c-4dbb-9fbd-6a2f6e7267de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image, ImageFile, ImageDraw\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "# Set random seed for replicability (MANUALLY)\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "# torch uses some non-deterministic algorithms\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3HVnwz0bWFi"
   },
   "source": [
    "### Runtime input variables\n",
    "\n",
    "Some inputs are defined:\n",
    "\n",
    "- **data_dir** - the root directory of the database, described later.\n",
    "- **num_workers** - the number of threads to load the data with the DataLoader class.\n",
    "- **batch_size** - the size of the _batch_.\n",
    "- **num_classes** - the number of classes to detect.\n",
    "- **class_names** - the names of the classes to detect.\n",
    "- **num_epochs** - number of _epochs_ for network training.\n",
    "- **step_size** - number of _epochs_ after which the learning rate is reduced by a factor of 0.1.\n",
    "- **lr** - initial learning rate.\n",
    "- **device** - the device (GPU or CPU) for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1652794430418,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "Z5R8rQ4-bWFj",
    "outputId": "4d33a27d-1664-4bd7-fb8f-b201894065ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"myVOC4\"\n",
    "num_workers=2         # to debug, fix num_workers=0\n",
    "batch_size = 1        # Training and test (if we use 1 we don't need to resize the images)\n",
    "num_classes=5         # Number of classes\n",
    "class_names=['background','bottle','chair','dining_table','sofa']\n",
    "num_epochs = 6\n",
    "step_size= 5\n",
    "lr=0.001\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ud-4uoN1bWFn"
   },
   "source": [
    "### Modified library \n",
    "\n",
    "For this session, a modified version of the torchvision library will be used, which we will name torchvision_05. This modified version allows to train our detection network with images that do not contain any objects, whereas the original version does not allow it. Likewise, it provides us with the information of the _anchors_ selected for each object during the inference. We import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 12400,
     "status": "ok",
     "timestamp": 1652794442812,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "RqIzieQbbWFn"
   },
   "outputs": [],
   "source": [
    "import torchvision_05\n",
    "from torchvision_05.models.detection.rpn import AnchorGenerator\n",
    "from torchvision_05.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
    "from torchvision_05.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-OXXBtIbWFs"
   },
   "source": [
    "### Network\n",
    "\n",
    "The network to be used follows the Faster-RCNN architecture using a ResNet-50 as backbone network. Specifically, the first 4 blocks of this network are used, composed of convolutional layers, batch normalization (whose parameters are frozen) and non-linear layers. We also add a FPN, _Feature Pyramid Network_, which extracts characteristics from the resulting maps of the network at different scales. After this *backbone*, Faster-RCNN's own modules are placed: the region proposition network and the _RoI pooling_ and classification.\n",
    "\n",
    "The following is the function that builds the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1652794442812,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "34rrV7QCbWFv"
   },
   "outputs": [],
   "source": [
    "def get_model_detection(num_classes):\n",
    "    # load a Faster RCNN model pre-trained on COCO\n",
    "    model = torchvision_05.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    " \n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67kuRikSbWF0"
   },
   "source": [
    "In the next cell, we will load the model, please stop to analyze each of the layers that define the model and its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "57684d533a1f49d49ec1e1b5f37685a9",
      "3c91568a14fd4b1bae563ab3674a9297",
      "f4b3f498409c47a18ce5deae4ebaa0bd",
      "f5060bc8e97f4351a2c09703b354c9a0",
      "3b2313b9f375419d8d7c49b925894329",
      "21c28b3b230e412dbb52142627f171b4",
      "af6762f368814ebeb2f791fe2a586a21",
      "5fb0958a2ab54f1088874def0d23908b",
      "5014560f097244f48b603a61aa2d6a16",
      "083017e8ddc64de0bcbda215a35d06b6",
      "e3e689b2c5db42e6a6567f17972b7465"
     ]
    },
    "executionInfo": {
     "elapsed": 13622,
     "status": "ok",
     "timestamp": 1652794456402,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "FBMIychIbWF0",
    "outputId": "bf750247-4d36-4a07-e1d0-3b4c3b03da50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform()\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=[0, 1, 2, 3], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_ft = get_model_detection(num_classes)\n",
    "print(model_ft)\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1O2DXZEbWF4"
   },
   "source": [
    "### Database\n",
    "\n",
    "The function to load the database is defined below. To simplify its treatment, the database is provided in the form of images (`images`) and masks for each of the object instances (` instances`). Thus, for each image there are as many masks as there are objects in it (with the segmentation of each one of them). Also, the category of each object is encoded in the name of the image. In addition, the `classes` semantic segmentation and the `masks` instance segmentation are provided.\n",
    "\n",
    "Try to understand the return variables of this function.\n",
    "\n",
    "**IMPORTANT**: input images are not normalized in this case. The _framework_ itself performs normalization internally with the mean of ImageNet samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1652794456404,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "VI55x40abWF5"
   },
   "outputs": [],
   "source": [
    "\n",
    "class myVOCDataset(object):\n",
    "    def __init__(self, root, train):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        \n",
    "\n",
    "        \n",
    "        if (self.train):\n",
    "            self.imgs = [os.path.join(root,name)\n",
    "                 for root, dirs, files in os.walk(root)\n",
    "                 for name in files if ('train'+os.path.sep+'images' in root)\n",
    "                 if name.lower().endswith(\".jpg\")]\n",
    "            #TOREMOVE - #DEBUG\n",
    "            maxSize=10\n",
    "            if maxSize>0:\n",
    "                self.imgs=self.imgs[0:maxSize]\n",
    "            ###\n",
    "        else:\n",
    "            self.imgs = [os.path.join(root,name)\n",
    "                 for root, dirs, files in os.walk(root)\n",
    "                 for name in files if ('test'+os.path.sep+'images' in root)\n",
    "                 if name.lower().endswith(\".jpg\")]\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = self.imgs[idx]\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Independent annotations for each object\n",
    "        file=self.imgs[idx].split(os.path.sep)[-1]\n",
    "        self.masks = [os.path.join(self.root,name)\n",
    "                        for self.root, dirs, files in os.walk(self.imgs[idx].replace('images','instances').replace(file,''))\n",
    "                        for name in files \n",
    "                        if (name.startswith(file[:-4]+'_'))]\n",
    "        if (not self.masks):\n",
    "            masks=np.array(Image.new('L', img.size))\n",
    "            labels=np.zeros((0,),dtype=np.int64)\n",
    "            num_objs=0\n",
    "        else:\n",
    "            mask=np.zeros((len(self.masks),img.size[1],img.size[0]),dtype=np.uint8)\n",
    "            labels=np.zeros((len(self.masks),),dtype=np.int64)\n",
    "            for j in range(len(self.masks)):\n",
    "                mask[j,:,:]=np.array(Image.open(self.masks[j]))\n",
    "                labels[j]=int(self.masks[j].split('_')[-1][:-4])\n",
    "            num_objs = len(mask)\n",
    "            masks=np.asarray(mask)\n",
    "                            \n",
    "        boxes = []\n",
    "\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            # Get aspect ratio\n",
    "            W=xmax-xmin\n",
    "            H=ymax-ymin\n",
    "            if (W==0 or H==0):\n",
    "                labels=np.delete(labels,i)\n",
    "            else:\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        num_objs = boxes.size(0)\n",
    "        # get the labels\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        # We need the area to filter the empty images\n",
    "        if (not boxes.size()==torch.Size([0])):\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        else:\n",
    "            area = torch.tensor([0.])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "\n",
    "        img=F.to_tensor(img)\n",
    "        return img, target, img_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQHqhLtrbWF-"
   },
   "source": [
    "We use the previously defined function to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1652794456926,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "bqhZicltbWGA"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = myVOCDataset(data_dir, train=True)\n",
    "dataset_test = myVOCDataset(data_dir, train=False)\n",
    "\n",
    "# define training and test data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, # to debug, fix num_workers=0\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Nf7S3EfbWGD"
   },
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "Likewise, the function that tests the quality of our model is defined in terms of object detection (precision and recall) and classification (among the detected objects, which ones are correctly classified according to their category).\n",
    "The function receives as parameters:\n",
    "- __model__: the CNN to evaluate.\n",
    "- __dataloader__: the loader of the test data.\n",
    "- __class_names__: names of the object classes to detect (always include the *background* class first).\n",
    "- __th_score__: the network provides each detected object with a probability between 0 and 1. This threshold discards objects with objectness such that $Pr(object)<score_{th}$.\n",
    "- __th_iou__: imposes a minimum IoU threshold from which to consider a detection as correct, that is, it is a decision on how to evaluate the system.\n",
    "- __result_dir__: path to the results directory.\n",
    "- __SAVE_OPT__: to save or not the test results as images with the * ground truth *, the predicted * bounding boxes * and the corresponding * anchors *.\n",
    "- __SAVE_FULL__: to save the total results on each image (True) or save them in different folders for each size of * anchor * for later analysis (False).\n",
    "- __VERBOSE__: prints on screen the total evaluation measures (without taking into account each class) if it is False, or the totals and marginals for each class if it is True.\n",
    "- __batchsize__: must be equal to 1.\n",
    "\n",
    "The function provides as outputs:\n",
    "- __precision_RPN__ - Precision for object detection of the RPN.\n",
    "- __recall_RPN__: the recall for the detection of RPN objects.\n",
    "- __f1_score_RPN__: the F1 score for detection, which we take as the main measure of system performance.\n",
    "- __cm_global__: confusion matrix between classes for classification.\n",
    "- __prec_rec_global__: global precision and recall for the multiclass detection problem.\n",
    "- __prec_rec_marginal__: per-class precision and recall for the multiclass detection problem.\n",
    "- __total_anchors__: all the anchors of the network for the test set, for further analysis.\n",
    "- __total_labels__: labels of said anchors.\n",
    "\n",
    "__IMPORTANT:__ It should be noted that the inference function returns the evaluation measures for both the RPN (`precision_RPN`,` recall_RPN` and `f1_score_RPN`), that is, these measures refer to the *objectness* or ability to detect objects from the network regardless of its category; and the final detection scores of the network (`cm_global`,` prec_rec_global` and `prec_rec_marginal`). Of course, errors in RPN are also transfered to the global metrics, decreasing the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1652795176834,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "jFgZZM3TbWGE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay, average_precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Auxiliary function to compute the intersection over union between 2 bounding boxes\n",
    "from external import bb_intersection_over_union\n",
    "\n",
    "\n",
    "def test_detection_model(model, dataloader, class_names, th_score, th_iou, result_dir, SAVE_OPT, SAVE_FULL, VERBOSE, batch_size=1):\n",
    "    since = time.time()\n",
    "    # Standard values for anchors and image size\n",
    "    bins=[48,96,192,384]\n",
    "    size_anchors=[32,64,128,256,512]\n",
    "    min_size = 800\n",
    "    max_size = 1333\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "    # Detection measurements\n",
    "    ret = 0\n",
    "    rel = np.zeros((len(class_names)-1,),dtype=int)\n",
    "    ret_rel = np.zeros((len(class_names)-1),dtype=int)\n",
    "    \n",
    "    # Classification measurements\n",
    "    y_true=np.zeros((0,),dtype='int')\n",
    "    y_pred=np.zeros((0,),dtype='int')\n",
    "    y_score=np.zeros((0,),dtype='int')\n",
    "    batch_counter = 0\n",
    "    total_anchors=np.zeros((0,4),dtype='float32')\n",
    "    total_labels=np.zeros((0,),dtype=int)\n",
    "    print('Evaluating...')\n",
    "    with torch.no_grad():\n",
    "        # Iterate over data.\n",
    "        for inputs, targets, paths in dataloader:\n",
    "            batch_counter = batch_counter + 1\n",
    "            inputs = list(image.to(device) for image in inputs)\n",
    "            labels = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            # Scale factor (useful for anchor extraction)\n",
    "            size_img=inputs[0].size()[1:]\n",
    "            scale_factor=min_size/min(size_img)\n",
    "            if max(size_img) * scale_factor > max_size:\n",
    "                scale_factor = max_size / max(size_img)\n",
    "            # Let's create the ground-truth boxes and labels\n",
    "            if (np.array(labels[0]['boxes'].cpu()).shape[0]==0):\n",
    "                gt_boxes=np.zeros((0,4),dtype='float32')\n",
    "            else:\n",
    "                gt_boxes=np.array(labels[0]['boxes'].detach().cpu())\n",
    "                gt_labels=np.array(labels[0]['labels'].detach().cpu())\n",
    "                # Relevant objects per class\n",
    "                for j in range(1,len(class_names)):\n",
    "                    rel[j-1]=rel[j-1]+np.sum(gt_labels==j)\n",
    "            # Prediction: returns boxes, scores (objectness), labels (class) and anchors\n",
    "            pred = model(inputs)\n",
    "            pred[0]['scores']=pred[0]['scores'].detach().cpu()\n",
    "            pred[0]['boxes']=pred[0]['boxes'].detach().cpu()\n",
    "            pred[0]['labels']=pred[0]['labels'].detach().cpu()\n",
    "            pred[0]['anchors']=pred[0]['anchors'].detach().cpu()\n",
    "            # We use only those with scores>th_score and convert them to numpy arrays\n",
    "            if (len(pred[0]['scores'].numpy())==0):\n",
    "                pred_boxes=np.zeros((0,4),dtype='float32')\n",
    "                pred_labels=np.zeros((0,),dtype=int)\n",
    "            else:\n",
    "                pred_score = list(pred[0]['scores'].numpy())\n",
    "                if (pred_score[0]>th_score):\n",
    "                    pred_t = [pred_score.index(x) for x in pred_score if x>th_score][-1]\n",
    "                    pred_class = [class_names[i] for i in list(pred[0]['labels'].numpy())]\n",
    "                    pred_labels = pred[0]['labels'].numpy()\n",
    "                    pred_boxes = [[i[0], i[1], i[2], i[3]] for i in list(pred[0]['boxes'].numpy())]\n",
    "                    pred_boxes = np.array(pred_boxes[:pred_t+1])\n",
    "                    pred_anchors = [[i[0], i[1], i[2], i[3]] for i in list(pred[0]['anchors'].numpy())]\n",
    "                    pred_anchors = np.array(pred_anchors[:pred_t+1])\n",
    "                    total_anchors = np.concatenate((total_anchors,pred_anchors),axis=0)\n",
    "                    pred_class = pred_class[:pred_t+1]\n",
    "                    pred_labels = pred_labels[:pred_t+1]\n",
    "                    total_labels = np.concatenate((total_labels,pred_labels),axis=0)\n",
    "                else:\n",
    "                    pred_boxes=np.zeros((0,4),dtype='float32')\n",
    "                    pred_labels=np.zeros((0,),dtype=int)\n",
    "\n",
    "                    \n",
    "            # Retrieved objects \n",
    "            ret=ret+len(pred_labels)\n",
    "            assigned=np.zeros((pred_boxes.shape[0],),dtype='float')\n",
    "            # Detection statistics: we compute the intersection over union between the ground-truth objects\n",
    "            # and the retrieved ones, if it exceed th_iou, the detection is considered as a good one\n",
    "            for j in range(gt_boxes.shape[0]):\n",
    "                y_true=np.concatenate((y_true,gt_labels[j][np.newaxis]),axis=0)\n",
    "                if pred_boxes.shape[0]>0:\n",
    "                    ious=np.zeros((pred_boxes.shape[0],),dtype='float')\n",
    "                \n",
    "                    for k in range(pred_boxes.shape[0]):\n",
    "                        ious[k]=bb_intersection_over_union(gt_boxes[j,:],pred_boxes[k,:])\n",
    "                    iou_max=np.max(ious)\n",
    "                    pos=np.argmax(ious)\n",
    "                else:\n",
    "                    iou_max=0\n",
    "                if (iou_max>th_iou):\n",
    "                    ret_rel[gt_labels[j]-1]+=1\n",
    "                    y_pred=np.concatenate((y_pred,pred_labels[pos][np.newaxis]),axis=0)\n",
    "                    assigned[pos]=1;\n",
    "                else:\n",
    "                    y_pred=np.concatenate((y_pred,np.zeros((1,))),axis=0)\n",
    "            #else:\n",
    "            #    y_pred=np.concatenate((y_pred,np.zeros((1,))),axis=0)\n",
    "            \n",
    "            #Now add non-assigned predictions as false detections to the statistics\n",
    "            for k in range(pred_boxes.shape[0]):\n",
    "                if assigned[k]==0:\n",
    "                    y_true=np.concatenate((y_true,np.zeros((1,))),axis=0)\n",
    "                    y_pred=np.concatenate((y_pred,pred_labels[k][np.newaxis]),axis=0)\n",
    "            \n",
    "\n",
    "            # Save the images with detections and ground-truth objects in a new folder, to analyze the results\n",
    "            if SAVE_OPT and SAVE_FULL:\n",
    "                aux=paths[0].split('/')\n",
    "                folder_path=os.path.join(result_dir,'instances_pred_full')\n",
    "                if not os.path.exists(folder_path):\n",
    "                    os.makedirs(folder_path)\n",
    "                img=np.array(F.to_pil_image(inputs[0].cpu()))\n",
    "                for i in range(pred_boxes.shape[0]):\n",
    "                    cv2.rectangle(img, (int(pred_boxes[i][0]),int(pred_boxes[i][1])), (int(pred_boxes[i][2]),int(pred_boxes[i][3])),color=(255, 0, 0), thickness=1)\n",
    "                    cv2.putText(img,pred_class[i], (int(pred_boxes[i][0]),int(pred_boxes[i][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),thickness=1)\n",
    "                for i in range(gt_boxes.shape[0]):\n",
    "                    cv2.rectangle(img, (gt_boxes[i][0],gt_boxes[i][1]), (gt_boxes[i][2],gt_boxes[i][3]),color=(0, 255, 0), thickness=1)\n",
    "                    cv2.putText(img,class_names[gt_labels[i]], (gt_boxes[i][0],gt_boxes[i][1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),thickness=1)\n",
    "                pred_path=folder_path+'/'+aux[-1]\n",
    "                cv2.imwrite(pred_path,cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "            elif SAVE_OPT and not SAVE_FULL:\n",
    "                aux=paths[0].split('/')\n",
    "                folder_path=os.path.join(result_dir,'instances_pred')\n",
    "                if not os.path.exists(folder_path):\n",
    "                    os.makedirs(folder_path)\n",
    "                for i in range(pred_boxes.shape[0]):\n",
    "                    anchor_size=size_anchors[np.digitize(np.sqrt((pred_anchors[i,2]-pred_anchors[i,0])*(pred_anchors[i,3]-pred_anchors[i,1])),bins)]\n",
    "                    img=np.array(F.to_pil_image(inputs[0].cpu()))\n",
    "                    cv2.rectangle(img, (int(pred_boxes[i][0]),int(pred_boxes[i][1])), (int(pred_boxes[i][2]),int(pred_boxes[i][3])),color=(255, 0, 0), thickness=1)\n",
    "                    cv2.rectangle(img, (int(pred_anchors[i][0]/scale_factor),int(pred_anchors[i][1]/scale_factor)), (int(pred_anchors[i][2]/scale_factor),int(pred_anchors[i][3]/scale_factor)), color=(0, 0, 255), thickness=1)\n",
    "                    cv2.putText(img,pred_class[i], (int(pred_boxes[i][0]),int(pred_boxes[i][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),thickness=1)\n",
    "                    for j in range(gt_boxes.shape[0]):\n",
    "                        cv2.rectangle(img, (int(gt_boxes[j][0]),int(gt_boxes[j][1])), (int(gt_boxes[j][2]),int(gt_boxes[j][3])),color=(0, 255, 0), thickness=1)\n",
    "                        cv2.putText(img,class_names[gt_labels[j]], (int(gt_boxes[j][0]),int(gt_boxes[j][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255),thickness=1)\n",
    "                    if (not os.path.exists(os.path.join(folder_path,str(anchor_size)))):\n",
    "                        os.makedirs(os.path.join(folder_path,str(anchor_size)))\n",
    "                    pred_path=folder_path+'/'+str(anchor_size)+'/'+aux[-1][:-4]+'_'+str(i)+'.png'\n",
    "                    cv2.imwrite(pred_path,cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Detection statistics: precision and recall per class\n",
    "    if (ret>0):\n",
    "        precision_RPN=np.sum(ret_rel)/ret\n",
    "    else:\n",
    "        precision_RPN=0\n",
    "    recall_RPN=np.zeros((rel.size,),dtype='float32')\n",
    "    for j in range(rel.size):\n",
    "        if (rel[j]>0):\n",
    "            recall_RPN[j]=ret_rel[j]/rel[j]\n",
    "        else:\n",
    "            recall_RPN[j]=0\n",
    "    # F1 score(weighted mean of precision and recall)\n",
    "    if (np.mean(recall_RPN)==0 or precision_RPN==0):\n",
    "        f1_score_RPN=0    \n",
    "    else:\n",
    "        f1_score_RPN=2*np.mean(recall_RPN)*precision_RPN/(np.mean(recall_RPN)+precision_RPN)\n",
    "    # Classification statistics: precision and recall\n",
    "    prec_rec_marginal=precision_recall_fscore_support(y_true, y_pred, average=None,labels=[f for f in range(1,len(class_names))],zero_division=0)\n",
    "    prec_rec_global=precision_recall_fscore_support(y_true, y_pred, average='macro',labels=[f for f in range(1,len(class_names))],zero_division=0)              \n",
    "    # Confusion matrix\n",
    "    cm_global=confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Evaluation complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Objectness-RPN. F1: {:4f}.     Precision: {:4f}. Recall: {:4f}'.format(f1_score_RPN,precision_RPN,np.mean(recall_RPN)))\n",
    "    if (VERBOSE):\n",
    "        for i in range(1,len(class_names)):\n",
    "            print('Class: ' + class_names[i]+'. Recall: {:1d}/{:1d}'.format(ret_rel[i-1],rel[i-1]))\n",
    "        print('')\n",
    "    if ((prec_rec_global[0]+prec_rec_global[1])>0):\n",
    "        f1_class=2*prec_rec_global[0]*prec_rec_global[1]/(prec_rec_global[0]+prec_rec_global[1])\n",
    "    else:\n",
    "        f1_class=0\n",
    "    print('Multi-class Detection: F1: {:4f}.     Precision: {:4f}. Recall: {:4f}'.format(f1_class,prec_rec_global[0],prec_rec_global[1]))\n",
    "    if (VERBOSE):\n",
    "        for i in range(1,len(class_names)):\n",
    "            if ((prec_rec_marginal[0][i-1]+prec_rec_marginal[1][i-1])>0):\n",
    "                f1_class=2*prec_rec_marginal[0][i-1]*prec_rec_marginal[1][i-1]/(prec_rec_marginal[0][i-1]+prec_rec_marginal[1][i-1])\n",
    "            else:\n",
    "                f1_class=0\n",
    "            print('Class: ' + class_names[i]+'. F1: {:4f}.     Precision: {:4f}. Recall: {:4f}'.format(f1_class,prec_rec_marginal[0][i-1],prec_rec_marginal[1][i-1]))\n",
    "    return (precision_RPN,recall_RPN,f1_score_RPN,cm_global,prec_rec_global,prec_rec_marginal,total_anchors,total_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZpCkDLRbWGG"
   },
   "source": [
    "### Training\n",
    "\n",
    "Training is performed during $n_e$ epochs, reducing the training error rate as training progresses. The code produces a file called _log.csv_ where the variation of the loss functions in each training _epoch_ can be analyzed, as well as the corresponding precision and recall in the test set for both detection and classification. Check that the loss functions decrease (approximately) monotonously, and that the test measures stagnate as the training progresses (and become louder).\n",
    "\n",
    "__IMPORTANT:__ the best _epoch_ of the network is considered as the one that provides the best F1-score measure (harmonic mean of precision and recall) and its weights are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50583,
     "status": "ok",
     "timestamp": 1652794508949,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "FQl1bKkzbWGI",
    "outputId": "f5241fd9-8dca-49a4-d435-ada466225f9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TRAIN) Epoch: [0]  [ 9/10]  eta: 0:00:15  lr: 0.001000  loss: 1.4829 (3.5528)  loss_classifier: 0.6691 (0.8135)  loss_box_reg: 0.0000 (0.0389)  loss_objectness: 0.0633 (2.0677)  loss_rpn_box_reg: 0.6429 (0.6327)  time: 15.0008  data: 0.0103\n",
      "(TRAIN) Epoch: [0] Total time: 0:02:30 (15.0014 s / it)\n",
      "2\n",
      "3\n",
      "(VAL) Epoch: [0]  [67/68]  eta: 0:00:05  loss: 15.4161 (16.3275)  loss_classifier: 0.1712 (0.2143)  loss_box_reg: 0.0888 (0.1165)  loss_objectness: 15.0726 (15.8297)  loss_rpn_box_reg: 0.0074 (0.1670)  time: 5.0546  data: 0.0136\n",
      "(VAL) Epoch: [0] Total time: 0:05:52 (5.1819 s / it)\n",
      "4\n",
      "5\n",
      "Evaluating...\n",
      "Evaluation complete in 6m 16s\n",
      "Objectness-RPN. F1: 0.000000.     Precision: 0.000000. Recall: 0.000000\n",
      "Multi-class Detection: F1: 0.000000.     Precision: 0.000000. Recall: 0.000000\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from engine import train_one_epoch, eval_one_epoch\n",
    "import time\n",
    "\n",
    "# We create the baseline folder\n",
    "result_dir=os.path.join(code_path,'baseline_results')\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "# Weights for L_objectness, L_reg, L_cls y L_reg2 \n",
    "weights=[1,1,1,1]\n",
    "\n",
    "# Training\n",
    "# build an optimizer\n",
    "params = [p for p in model_ft.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=lr,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=step_size,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# CSV file for training results\n",
    "if os.path.exists(os.path.join(result_dir,'log.csv')):\n",
    "    os.remove(os.path.join(result_dir,'log.csv'))\n",
    "csv_file=open(os.path.join(os.path.join(result_dir,'log.csv')),'w')\n",
    "coord_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "coord_writer.writerow(['Train total','Train rpn_box_reg','Train objectness','Train box_reg','Train classifier',\n",
    "                        'Val total','Val rpn_box_reg','Val objectness','Val box_reg','Val classifier'])\n",
    "best_f1=-0.0001\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: \" + str(epoch+1) + \"/\" + str(num_epochs))\n",
    "    if not os.path.exists(os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch))):\n",
    "        #     train for one epoch, printing every epoch \n",
    "        train_aux_losses=train_one_epoch(model_ft, optimizer, data_loader, device, weights, epoch, print_freq=250)\n",
    "        # Update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # Validation\n",
    "        val_aux_losses=eval_one_epoch(model_ft, data_loader_test, device, epoch, print_freq=250)\n",
    "        # Save the losses\n",
    "        coord_writer.writerow([str(train_aux_losses['total']),str(train_aux_losses['rpn_box_reg']),str(train_aux_losses['objectness']),\n",
    "                                str(train_aux_losses['box_reg']),str(train_aux_losses['classifier']),\n",
    "                                str(val_aux_losses['total']),str(val_aux_losses['rpn_box_reg']),str(val_aux_losses['objectness']),\n",
    "                                str(val_aux_losses['box_reg']),str(val_aux_losses['classifier'])])\n",
    "        # Evaluation\n",
    "        [precision,recall,f1_score,cm,total,partial,_,_]=test_detection_model(model_ft, data_loader_test, class_names, 0.5,0.5, result_dir, False, False, False)\n",
    "        # Save the state and the model with best AP-score for inference\n",
    "        state = {'epoch': epoch + 1, 'state_dict': model_ft.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict(),\n",
    "                  'scheduler':lr_scheduler.state_dict(),}\n",
    "        torch.save(state, os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch)))\n",
    "        if (f1_score>best_f1):\n",
    "            torch.save(state, os.path.join(result_dir,'best_model.pth'.format(epoch)))\n",
    "            best_f1=f1_score\n",
    "    else:\n",
    "        # Load this epoch information to resume training\n",
    "        print(\"=> loading checkpoint '{}'\".format(epoch))\n",
    "        checkpoint = torch.load(os.path.join(result_dir,'faster-rcnn-epoch{}.pth'.format(epoch)))\n",
    "        lr_scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        model_ft.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}'\" .format(epoch))\n",
    "    \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5jVIptizwyv"
   },
   "source": [
    "If the execution is halted mid-training for any reason, make sure to close the csv file that was opened (uncomment following cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1652794508949,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "aDv8NAhfz4cd"
   },
   "outputs": [],
   "source": [
    "# csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P-Dx8U-bWGL"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "After training the network, the results for the test set will be obtained. First, the trained model is loaded and the evaluation function is called. Pay attention to the parameters that the function receives.\n",
    "\n",
    "Analyze the information that the function returns. Notice the difference between two performance metrics: a) the measures of *objectness* on the RPN and b) the system's object classification measures (over candidate bounding boxes considered as positive by the RPN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33512,
     "status": "ok",
     "timestamp": 1652794542428,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "9pPgRCnMbWGM",
    "outputId": "296686ff-fae4-4cf9-de88-850b93beab90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Evaluation complete in 5m 1s\n",
      "Objectness-RPN. F1: 0.000000.     Precision: 0.000000. Recall: 0.000000\n",
      "Class: bottle. Recall: 0/36\n",
      "Class: chair. Recall: 0/52\n",
      "Class: dining_table. Recall: 0/15\n",
      "Class: sofa. Recall: 0/27\n",
      "\n",
      "Multi-class Detection: F1: 0.000000.     Precision: 0.000000. Recall: 0.000000\n",
      "Class: bottle. F1: 0.000000.     Precision: 0.000000. Recall: 0.000000\n",
      "Class: chair. F1: 0.000000.     Precision: 0.000000. Recall: 0.000000\n",
      "Class: dining_table. F1: 0.000000.     Precision: 0.000000. Recall: 0.000000\n",
      "Class: sofa. F1: 0.000000.     Precision: 0.000000. Recall: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "model_weights = torch.load(os.path.join(result_dir,'best_model.pth'.format(num_epochs-1)))['state_dict']\n",
    "model_ft.load_state_dict(model_weights)\n",
    "results=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, False, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJ6thL68bWGQ"
   },
   "source": [
    "## Part 3. Experiments.\n",
    "\n",
    "This section will contain the experiments and analysis implemented by the student. These, along with the answers to the questions proposed during the section, will be used to evaluate the student's knowledge on the topic of object detection and, therefore, will determine their grade for this laboratory.\n",
    "\n",
    "Note that implementation answers should be provided in the empty code cells whereas textual justification should be written in the empty text cells. You can generate additional cells (both text and code) if you deem it necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxD_NSBJsYIp"
   },
   "source": [
    "### 1. Baseline evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8bxxc8zs2wj"
   },
   "source": [
    "Discuss why values of F1-score in Multiclass-detection are lower than those of Objectness-RPN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrIC_aWv-zV0"
   },
   "source": [
    "> Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GquRQGkw_XAW"
   },
   "source": [
    "Use the following code to show the normaized confussion matrix and discuss the results. Why background class has a 0 in the diagonal? Without considering the background class, which classes are harder to distinguish for the detection model? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1652794542706,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "aHC6izM__bZJ",
    "outputId": "4c9767aa-4eef-4405-cf69-36070d80a8e4"
   },
   "outputs": [],
   "source": [
    "cm_global=results[3]\n",
    "cm_global=cm_global/np.sum(cm_global,axis=1,keepdims=True)\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=cm_global,display_labels=class_names)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRDMaE1qbWGQ"
   },
   "source": [
    "### 2. Influence of the threshold on the results.\n",
    "\n",
    "#### __*Objectness threshold*__\n",
    "\n",
    "Analize the influence of the objectness score threshold on the results of the model. To this end, you can make use of the `test_detection_model` function to check three thresholds (0.3, 0.5, 0.7). Analyze the global precisio, recall and F1-score values and discuss if they make sense.\n",
    "\n",
    "You can also support your analysis with deeper insight (marginal inference can be shown through the `VERBOSE` parameter) and visual aid (use `SAVE_OPT` to that end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-sqbQCbdG_j"
   },
   "source": [
    "Which threshold provides the best results? Do you think that this would be the case for all object detection problems? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100924,
     "status": "ok",
     "timestamp": 1652795031146,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "LE_UeMsAD4Q5",
    "outputId": "c4276158-8632-4270-fa80-0e501b4e3e30"
   },
   "outputs": [],
   "source": [
    "# Include your code here\n",
    "print('Threshold 0.3')\n",
    "results=test_detection_model(model_ft, data_loader_test, class_names, 0.3, 0.5, result_dir, True, False, True)\n",
    "#!mv baseline_results/instances_pred baseline_results/instances_pred_0_3\n",
    "# Include your code here\n",
    "print('Threshold 0.5')\n",
    "results=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, True, False, True)\n",
    "#!mv baseline_results/instances_pred baseline_results/instances_pred_0_5\n",
    "# Include your code here\n",
    "print('Threshold 0.7')\n",
    "results=test_detection_model(model_ft, data_loader_test, class_names, 0.7, 0.5, result_dir, True, False, True)\n",
    "#!mv baseline_results/instances_pred baseline_results/instances_pred_0_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvnR9uAzd15X"
   },
   "source": [
    "> Answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE05lYipbWGV"
   },
   "source": [
    "#### __*IoU threshold*__\n",
    "\n",
    "Analyze the influence of the IoU threshold on the results. To this end, you can make use of the `test_detection_model` function to check three thresholds (0.3, 0.5, 0.7). You can support your analysis with deeper insight (marginal inference can be shown through the `VERBOSE` parameter) and visual aid (use `SAVE_OPT` to that end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99970,
     "status": "ok",
     "timestamp": 1652795300986,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "Mr2hU0qmbWGV",
    "outputId": "44b81dee-e6b5-4041-f402-50b470c0126b"
   },
   "outputs": [],
   "source": [
    "# Include your code here\n",
    "print('IoU_TH 0.3')\n",
    "results=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.3, result_dir, True, False, True)\n",
    "#!mv baseline_results/instances_pred baseline_results/instances_pred_iou_0_3\n",
    "# Include your code here\n",
    "print('IoU_TH 0.5')\n",
    "results=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, True, False, True)\n",
    "#!mv baseline_results/instances_pred baseline_results/instances_pred_iou_0_5\n",
    "# Include your code here\n",
    "print('IoU_TH 0.7')\n",
    "results=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.7, result_dir, True, False, True)\n",
    "#!mv baseline_results/instances_pred baseline_results/instances_pred_iou_0_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n8JhZ3xgaGO"
   },
   "source": [
    "Go back to the implementation part and analyze the code. Which were the values used for the objectness and IoU thresholds during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsh4ylvUg4kW"
   },
   "source": [
    "> Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmlWE9gBbWGY"
   },
   "source": [
    "### 3. Analysis of the anchor sizes .\n",
    "\n",
    "The images in the PASCAL VOC database have a maximum size of 500 pixels on their longest side and one smaller side of variable size, and therefore different aspect ratios. Note, however, that Pytorch scales images so that their minimum side measures 800 pixels or their maximum side 1333 (see the __resize__ function of __torchvision_05.models.detection.transform__). \n",
    "\n",
    "The following code (when uncommented) calculates the distribution of sizes and aspect ratios of objects in the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 766
    },
    "executionInfo": {
     "elapsed": 5197,
     "status": "ok",
     "timestamp": 1652796261679,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "VrcISn3xbWGZ",
    "outputId": "6ad1cdd6-be32-4490-d2f3-e1bb2be23752"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from external import get_gt_anchors\n",
    "# define training and test data loaders\n",
    "data_loader_0 = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test_0 = torch.utils.data.DataLoader(\n",
    "     dataset_test, batch_size=batch_size, shuffle=False, num_workers=0,\n",
    "     collate_fn=utils.collate_fn)\n",
    "\n",
    "ar_bins=[0.25,0.5,1,2,4]\n",
    "log_ar_bins=np.log(ar_bins)\n",
    "\n",
    "# TRAIN\n",
    "ar, labels, size_anchors=get_gt_anchors(data_loader_0)\n",
    "#We pass to logarithm to get linear distribution in x-axis\n",
    "log_ar=np.log(ar)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2,figsize=(12,12))\n",
    "ax0, ax1, ax2, ax3 = axes.flatten()\n",
    "ax0.hist([log_ar[np.where(labels==1)[0]],log_ar[np.where(labels==2)[0]],log_ar[np.where(labels==3)[0]],log_ar[np.where(labels==4)[0]]],\n",
    "         bins=log_ar_bins,color=['blue','orange','green','purple'],label=class_names[1:])\n",
    "\n",
    "ax0.legend(prop={'size': 10})\n",
    "ax0.set_xticks(log_ar_bins)\n",
    "ax0.set_xticklabels(ar_bins)\n",
    "ax0.set_title('Aspect ratio, train')\n",
    "ax1.hist([size_anchors[np.where(labels==1)[0]],size_anchors[np.where(labels==2)[0]],size_anchors[np.where(labels==3)[0]],size_anchors[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
    "ax1.legend(prop={'size': 10})\n",
    "ax1.set_title('Box size, train')\n",
    "\n",
    "# TEST\n",
    "ar, labels, size_anchors=get_gt_anchors(data_loader_test_0)\n",
    "log_ar=np.log(ar)\n",
    "ax2.hist([log_ar[np.where(labels==1)[0]],log_ar[np.where(labels==2)[0]],log_ar[np.where(labels==3)[0]],log_ar[np.where(labels==4)[0]]],\n",
    "         bins=log_ar_bins,color=['blue','orange','green','purple'],label=class_names[1:])\n",
    "ax2.legend(prop={'size': 10})\n",
    "ax2.set_title('Aspect ratio, test')\n",
    "ax2.set_xticks(log_ar_bins)\n",
    "ax2.set_xticklabels(ar_bins)\n",
    "ax3.hist([size_anchors[np.where(labels==1)[0]],size_anchors[np.where(labels==2)[0]],size_anchors[np.where(labels==3)[0]],size_anchors[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
    "ax3.legend(prop={'size': 10})\n",
    "ax3.set_title('Box size, test')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns4s3ld0bWGb"
   },
   "source": [
    "The following one, on the other hand, calculates (once uncommented) the distribution of the anchors provided by the net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "executionInfo": {
     "elapsed": 29312,
     "status": "ok",
     "timestamp": 1652796291715,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "4e71ZhRmbWGc",
    "outputId": "e09026d6-511f-4c8f-a69b-12f782c60c3e"
   },
   "outputs": [],
   "source": [
    "# Inference: extract the selected anchors\n",
    "model_weights = torch.load(os.path.join(result_dir,'best_model.pth'.format(num_epochs-1)))['state_dict']\n",
    "model_ft.load_state_dict(model_weights)\n",
    "[_,_,_,_,_,_,anchors,l_t]=test_detection_model(model_ft, data_loader_test, class_names, 0.5, 0.5, result_dir, False, False, False)\n",
    "\n",
    "# Anchor statistics\n",
    "ar=[]\n",
    "labels=[]\n",
    "size_anchors=[]\n",
    "for i in range(anchors.shape[0]):\n",
    "    labels.append(l_t[i])\n",
    "    size_anchors.append(np.sqrt((anchors[i,2]-anchors[i,0])*(anchors[i,3]-anchors[i,1])))\n",
    "    ar.append((anchors[i,2]-anchors[i,0])/(anchors[i,3]-anchors[i,1]))\n",
    "\n",
    "ar_bins=[0.25,0.5,1,2,4]\n",
    "log_ar_bins=np.log(ar_bins)\n",
    "\n",
    "ar=np.array(ar)\n",
    "log_ar=np.log(ar)\n",
    "labels=np.array(labels)\n",
    "size_anchors=np.array(size_anchors)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,12))\n",
    "ax0, ax1 = axes.flatten()\n",
    "ax0.hist([log_ar[np.where(labels==1)[0]],log_ar[np.where(labels==2)[0]],log_ar[np.where(labels==3)[0]],log_ar[np.where(labels==4)[0]]],\n",
    "         bins=log_ar_bins,color=['blue','orange','green','purple'],label=class_names[1:])\n",
    "ax0.legend(prop={'size': 10})\n",
    "ax0.set_xticks(log_ar_bins)\n",
    "ax0.set_xticklabels(ar_bins)\n",
    "ax0.set_title('Aspect ratio, train')\n",
    "ax1.hist([size_anchors[np.where(labels==1)[0]],size_anchors[np.where(labels==2)[0]],size_anchors[np.where(labels==3)[0]],size_anchors[np.where(labels==4)[0]]],color=['blue','orange','green','purple'],label=class_names[1:])\n",
    "ax1.legend(prop={'size': 10})\n",
    "ax1.set_title('Anchor size, train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaM15KPTjosV"
   },
   "source": [
    "Is the distribution of the anchors' sizes and shapes related to those of the bounding boxes provided in training and testing? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coq_JDrJkKBG"
   },
   "source": [
    "> Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jULUd0lvkM70"
   },
   "source": [
    "After analyzing these results, discuss why the library changes the size of the images when these are fed to the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kDVHe4skdyY"
   },
   "source": [
    "> Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01GlZ4GCkgdO"
   },
   "source": [
    "Provide a brief analysis of the visual output of the net, the anchors and the bounding boxes obtained through regression for a given image (you can choose some examples that help you to discuss the results).\n",
    "  \n",
    "Does regression work well for all cases? Does it work better for larger or smaller objects? Why do you think that happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omH1PUQvp5X3"
   },
   "source": [
    "> Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P51uzshNbWGg"
   },
   "source": [
    "### 4. _RoI pooling_ and classification.\n",
    "\n",
    "In this section, a visual representation of the *RoI pooling* outputs for the different classes will be obtained using the external function `visualize_roipooling`. Furthermore, the classification results will be analyzed. Uncomment the next cell to perform the analysis.\n",
    "\n",
    "In the following code cell, visual representations of the *RoI pooling* are extracted for each object detected in the `roipool` folder, inside the results folder. The *RoI pooling* outputs a feature map $F_r$ of size $H\\times W\\times C = 7\\times 7\\times 2048$. Each representation, in turn, corresponds to a 7x7 cell in which the maximum value of this map $F_r$ is represented over all channels (that is, each element contains the maximum value of all channels for each cell). In this way, the spatial distribution of the activations in the *RoI pooling* can be inferred. Run the following cell and analyze the output representations.\n",
    "\n",
    "__IMPORTANT__: the mean is not used to extract the representations since in convolutional networks most of the values ​​of the channels in a certain position are usually very low, and this makes that the mean values ​​are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25623,
     "status": "ok",
     "timestamp": 1652796317317,
     "user": {
      "displayName": "IVAN GONZALEZ DIAZ",
      "userId": "08110117324614430978"
     },
     "user_tz": -120
    },
    "id": "A9ivq3FfbWGi",
    "outputId": "6fa73def-3124-4e21-fbd3-3a54d82d1cc3"
   },
   "outputs": [],
   "source": [
    "del sys.modules['external']\n",
    "from external import visualize_roipooling\n",
    "\n",
    "# For the roi-pooling experiment, analyze the top bounding-boxes (th_score>0.7) and only those with \n",
    "# th_iou>0.7 with the ground-truth\n",
    "visualize_roipooling(model_ft, data_loader_test, device, class_names, 0.7, 0.7, result_dir, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkDgstUErV30"
   },
   "source": [
    "Visualize the outputs provided by `visualize_roipooling` (folder roi pooling) and focus on the objects that appear completely in the images. Can the shape of the object (and perhaps its category) be inferred from the spatial distribution of the RoI pooling regardless of its scale? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk3oSn1Brsfc"
   },
   "source": [
    "> Answer here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "s7 - ObjectDetection_students.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "083017e8ddc64de0bcbda215a35d06b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21c28b3b230e412dbb52142627f171b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b2313b9f375419d8d7c49b925894329": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c91568a14fd4b1bae563ab3674a9297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21c28b3b230e412dbb52142627f171b4",
      "placeholder": "​",
      "style": "IPY_MODEL_af6762f368814ebeb2f791fe2a586a21",
      "value": "100%"
     }
    },
    "5014560f097244f48b603a61aa2d6a16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "57684d533a1f49d49ec1e1b5f37685a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c91568a14fd4b1bae563ab3674a9297",
       "IPY_MODEL_f4b3f498409c47a18ce5deae4ebaa0bd",
       "IPY_MODEL_f5060bc8e97f4351a2c09703b354c9a0"
      ],
      "layout": "IPY_MODEL_3b2313b9f375419d8d7c49b925894329"
     }
    },
    "5fb0958a2ab54f1088874def0d23908b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af6762f368814ebeb2f791fe2a586a21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3e689b2c5db42e6a6567f17972b7465": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4b3f498409c47a18ce5deae4ebaa0bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fb0958a2ab54f1088874def0d23908b",
      "max": 167502836,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5014560f097244f48b603a61aa2d6a16",
      "value": 167502836
     }
    },
    "f5060bc8e97f4351a2c09703b354c9a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_083017e8ddc64de0bcbda215a35d06b6",
      "placeholder": "​",
      "style": "IPY_MODEL_e3e689b2c5db42e6a6567f17972b7465",
      "value": " 160M/160M [00:01&lt;00:00, 125MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
